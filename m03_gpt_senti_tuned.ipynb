{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":490,"status":"ok","timestamp":1718266520272,"user":{"displayName":"오흥록Edward","userId":"04874743833885656665"},"user_tz":-540},"id":"ZZu7K_2_qGRU","outputId":"bc2e6441-6fb1-4102-a3e8-679f6d4408e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["openai 1.35.7\n"]}],"source":["!openai --version"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":408,"status":"ok","timestamp":1718266703332,"user":{"displayName":"오흥록Edward","userId":"04874743833885656665"},"user_tz":-540},"id":"CCC630Ire_4P"},"outputs":[],"source":["from openai import OpenAI\n","import os\n","import pandas as pd\n","import time\n","import yaml\n","import re\n","import ipywidgets as widgets # interactive display\n","from tqdm import tqdm # progress bar\n","from dotenv import load_dotenv\n","import json"]},{"cell_type":"markdown","metadata":{},"source":["# ENV, Config 파일 읽기"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# .env 파일에서 환경 변수 로드(API 키)\n","load_dotenv()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# YAML 파일 열기\n","yaml_path = 'config.yaml' # todo: config 파일과 합치기\n","\n","with open(yaml_path, 'r') as f:\n","    config = yaml.safe_load(f)"]},{"cell_type":"markdown","metadata":{},"source":["# 객체 생성"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"]},{"cell_type":"markdown","metadata":{},"source":["# 함수 정의"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def get_response(messages):\n","    \"\"\"\n","    model: 모델 종류\n","    messages: 사용자의 입력과 모델의 출력을 교환하는 메시지 목록\n","    max_tokens: 생성될 응답의 최대 길이\n","    temperature: 생성될 응답의 다양성(0.0 ~ 1.0) 0.0은 가장 확실한 답변을, 1.0은 가장 다양한 답변을 생성\n","    stream: 응답을 한 번에 반환할지 여부. False로 설정하면 한 번에 반환\n","    \"\"\"\n","    response = client.chat.completions.create(\n","        model = \"gpt-4o\",\n","        messages = messages,\n","        # max_tokens = 150,\n","        temperature = 0.0,\n","        stream = False) \n","    return response.choices[0].message.content"]},{"cell_type":"markdown","metadata":{},"source":["# 파일 읽기"]},{"cell_type":"markdown","metadata":{},"source":["# test 영역"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["'/Users/heungrokoh/python_workspace/work/voucher_paprika'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["pwd()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tid(S사)</th>\n","      <th>raw_text</th>\n","      <th>theme</th>\n","      <th>issue</th>\n","      <th>sentiment</th>\n","      <th>sentiment(all)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>FB-7</td>\n","      <td>자취하는데 뭔 티비야 .. 하면서 노트북이나 패드로 유튜브, ott 시청했었는데 확...</td>\n","      <td>화면 크기, 이동 편리성, 삶의 질</td>\n","      <td>큰 화면으로 OTT 시청, 이동 편리, 삶의 질 향상</td>\n","      <td>화면 크기: 긍정, 이동 편리성: 긍정, 삶의 질: 긍정</td>\n","      <td>긍정</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>FB-8</td>\n","      <td>핸드폰으로 ott를 보다보니 항상 아쉬웠는데 고민고민하다질렀어요~^^ 주문후 다음날...</td>\n","      <td>배송, 디자인, 품질</td>\n","      <td>빠른 배송, OTT 시청, 디자인 만족, 품질 만족</td>\n","      <td>이동: 긍정, 사운드: 긍정, 화질: 긍정</td>\n","      <td>긍정</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>FB-15</td>\n","      <td>너무 좋아요무겁지만 바퀴가 있어 이동이 용이 하고사운드,화질등 나무랄게 없어요</td>\n","      <td>이동, 사운드, 화질</td>\n","      <td>무겁지만 이동 용이, 우수한 사운드, 우수한 화질</td>\n","      <td>이동: 긍정, 사운드: 긍정, 화질: 긍정</td>\n","      <td>긍정</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>FB-16</td>\n","      <td>밥 먹을때 자기전에 이동하면서 보고있는데 너무 편하고 좋아요</td>\n","      <td>사용성</td>\n","      <td>이동 편리, 식사시간 시청, 자기전 시청</td>\n","      <td>사용성: 긍정</td>\n","      <td>긍정</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>FB-29</td>\n","      <td>ott 유튜브만 보는분들에게 적극추천 리모컨으로도 터치로도 작동 잘됩니다~</td>\n","      <td>OTT/유튜브, 리모컨, 터치</td>\n","      <td>OTT/유튜브 시청, 리모컨, 터치 작동</td>\n","      <td>OTT/유튜브: 긍정, 리모컨: 긍정, 터치: 긍정</td>\n","      <td>긍정</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  tid(S사)                                           raw_text  \\\n","0    FB-7  자취하는데 뭔 티비야 .. 하면서 노트북이나 패드로 유튜브, ott 시청했었는데 확...   \n","1    FB-8  핸드폰으로 ott를 보다보니 항상 아쉬웠는데 고민고민하다질렀어요~^^ 주문후 다음날...   \n","2   FB-15        너무 좋아요무겁지만 바퀴가 있어 이동이 용이 하고사운드,화질등 나무랄게 없어요   \n","3   FB-16                  밥 먹을때 자기전에 이동하면서 보고있는데 너무 편하고 좋아요   \n","4   FB-29          ott 유튜브만 보는분들에게 적극추천 리모컨으로도 터치로도 작동 잘됩니다~   \n","\n","                 theme                          issue  \\\n","0  화면 크기, 이동 편리성, 삶의 질  큰 화면으로 OTT 시청, 이동 편리, 삶의 질 향상   \n","1          배송, 디자인, 품질   빠른 배송, OTT 시청, 디자인 만족, 품질 만족   \n","2          이동, 사운드, 화질    무겁지만 이동 용이, 우수한 사운드, 우수한 화질   \n","3                  사용성         이동 편리, 식사시간 시청, 자기전 시청   \n","4     OTT/유튜브, 리모컨, 터치         OTT/유튜브 시청, 리모컨, 터치 작동   \n","\n","                         sentiment sentiment(all)  \n","0  화면 크기: 긍정, 이동 편리성: 긍정, 삶의 질: 긍정             긍정  \n","1          이동: 긍정, 사운드: 긍정, 화질: 긍정             긍정  \n","2          이동: 긍정, 사운드: 긍정, 화질: 긍정             긍정  \n","3                          사용성: 긍정             긍정  \n","4     OTT/유튜브: 긍정, 리모컨: 긍정, 터치: 긍정             긍정  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv('data/trainset_fine_tuning.csv')\n","train.head()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["system_messages = \"\"\"넌 글을 요약하는 전문가야. 내용을 입력하면 아래 지시사항에 따라 요약해줘.\n","지시사항:\n","1. 원문을 입력하면 주요 테마를 추출해줘.\n","2. 테마와 관련된 구체적인 이슈나 관심사를 나열해줘.\n","3. 테마별 감성 분석 결과를 나열해줘. 각 테마에 대해 매우 긍정, 긍정, 중립, 부정, 매우 부정 중 하나로 표현.\n","4. 원문 전체에 대한 감성 분석 결과를 매우 긍정, 긍정, 중립, 부정, 매우 부정 중 하나로 표현.\n","5. 결과를 포맷에 맞춰 출력해.\n","\n","결과 포맷: \n","[원문]\n","원문 텍스트\n","\n","[theme]\n","원문에서 추출할 주요 테마를 나열\n","\n","[issue]\n","테마와 관련된 구체적인 이슈나 관심사를 나열\n","\n","[sentiment]\n","테마별 감성 분석 결과를 나열. 각 테마에 대해 매우 긍정, 긍정, 중립, 부정, 매우 부정 중 하나로 표현\n","\n","[sentiment(all)]\n","원문 전체에 대한 감성 분석 결과를 매우 긍정, 긍정, 중립, 부정, 매우 부정 중 하나로 표현\"\"\""]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'system_message': '넌 글을 요약하는 전문가야. 내용을 입력하면 아래 지시사항에 따라 요약해줘.\\n지시사항:\\n1. 원문을 입력하면 주요 테마를 추출해줘.\\n2. 테마와 관련된 구체적인 이슈나 관심사를 나열해줘.\\n3. 테마별 감성 분석 결과를 나열해줘. 각 테마에 대해 매우 긍정, 긍정, 중립, 부정, 매우 부정 중 하나로 표현.\\n4. 원문 전체에 대한 감성 분석 결과를 매우 긍정, 긍정, 중립, 부정, 매우 부정 중 하나로 표현.\\n5. 결과를 포맷에 맞춰 출력해.\\n\\n결과 포맷: \\n[원문]\\n원문 텍스트\\n\\n[theme]\\n원문에서 추출할 주요 테마를 나열\\n\\n[issue]\\n테마와 관련된 구체적인 이슈나 관심사를 나열\\n\\n[sentiment]\\n테마별 감성 분석 결과를 나열. 각 테마에 대해 매우 긍정, 긍정, 중립, 부정, 매우 부정 중 하나로 표현\\n\\n[sentiment(all)]\\n원문 전체에 대한 감성 분석 결과를 매우 긍정, 긍정, 중립, 부정, 매우 부정 중 하나로 표현', 'user_message': '[원문]\\n자취하는데 뭔 티비야 .. 하면서 노트북이나 패드로 유튜브, ott 시청했었는데 확실히 더 큰 화면으로 보니까 너무 조아요설거지하거나 가구 배치 바꿀 때도 쉽게 움직여서 이동시킬 수 있으니까 정말 좋습니다..삶의 질이 향상됐어요 ㅎㅎ\\n[/원문]', 'assistant_message': '[원문]\\n자취하는데 뭔 티비야 .. 하면서 노트북이나 패드로 유튜브, ott 시청했었는데 확실히 더 큰 화면으로 보니까 너무 조아요설거지하거나 가구 배치 바꿀 때도 쉽게 움직여서 이동시킬 수 있으니까 정말 좋습니다..삶의 질이 향상됐어요 ㅎㅎ\\n[/원문]\\n[theme]\\n화면 크기, 이동 편리성, 삶의 질\\n[/theme]\\n[issue]\\n큰 화면으로 OTT 시청, 이동 편리, 삶의 질 향상\\n[/issue]\\n[sentiment]\\n화면 크기: 긍정, 이동 편리성: 긍정, 삶의 질: 긍정\\n[/sentiment]\\n[sentiment(all)]\\n긍정\\n[/sentiment(all)]'}\n","                                      system_message  \\\n","0  넌 글을 요약하는 전문가야. 내용을 입력하면 아래 지시사항에 따라 요약해줘.\\n지시...   \n","1  넌 글을 요약하는 전문가야. 내용을 입력하면 아래 지시사항에 따라 요약해줘.\\n지시...   \n","2  넌 글을 요약하는 전문가야. 내용을 입력하면 아래 지시사항에 따라 요약해줘.\\n지시...   \n","3  넌 글을 요약하는 전문가야. 내용을 입력하면 아래 지시사항에 따라 요약해줘.\\n지시...   \n","4  넌 글을 요약하는 전문가야. 내용을 입력하면 아래 지시사항에 따라 요약해줘.\\n지시...   \n","\n","                                        user_message  \\\n","0  [원문]\\n자취하는데 뭔 티비야 .. 하면서 노트북이나 패드로 유튜브, ott 시청...   \n","1  [원문]\\n핸드폰으로 ott를 보다보니 항상 아쉬웠는데 고민고민하다질렀어요~^^ 주...   \n","2  [원문]\\n너무 좋아요무겁지만 바퀴가 있어 이동이 용이 하고사운드,화질등 나무랄게 ...   \n","3     [원문]\\n밥 먹을때 자기전에 이동하면서 보고있는데 너무 편하고 좋아요\\n[/원문]   \n","4  [원문]\\nott 유튜브만 보는분들에게 적극추천 리모컨으로도 터치로도 작동 잘됩니다...   \n","\n","                                   assistant_message  \n","0  [원문]\\n자취하는데 뭔 티비야 .. 하면서 노트북이나 패드로 유튜브, ott 시청...  \n","1  [원문]\\n핸드폰으로 ott를 보다보니 항상 아쉬웠는데 고민고민하다질렀어요~^^ 주...  \n","2  [원문]\\n너무 좋아요무겁지만 바퀴가 있어 이동이 용이 하고사운드,화질등 나무랄게 ...  \n","3  [원문]\\n밥 먹을때 자기전에 이동하면서 보고있는데 너무 편하고 좋아요\\n[/원문]...  \n","4  [원문]\\nott 유튜브만 보는분들에게 적극추천 리모컨으로도 터치로도 작동 잘됩니다...  \n"]}],"source":["# data폴더의 jsonl 파일 읽기\n","jsonl_path = 'data/fine_tuning_data.jsonl'\n","data = []\n","with open(jsonl_path, 'r', encoding='utf-8') as file:\n","    for line in file:\n","        data.append(json.loads(line))\n","\n","# 데이터 출력 (예: 첫 번째 항목)\n","print(data[0])\n","\n","# 또는 pandas DataFrame으로 변환\n","df = pd.DataFrame(data)\n","\n","# DataFrame을 출력하여 쉽게 확인\n","print(df.head())\n"]},{"cell_type":"markdown","metadata":{},"source":["# 훈련파일 업로드"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["FileObject(id='file-KesfDbwQWsK19Rd9FNpYmm3a', bytes=28197, created_at=1725358653, filename='fine_tuning_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["client.files.create(\n","    file=open('data/fine_tuning_data.jsonl', 'rb'),\n","    purpose='fine-tune'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}"]},{"cell_type":"markdown","metadata":{},"source":["# Fine-tuning"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"ename":"BadRequestError","evalue":"Error code: 400 - {'error': {'message': 'Model gpt-4o is not available for fine-tuning or does not exist.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfine_tuning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile-KesfDbwQWsK19Rd9FNpYmm3a\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pap/lib/python3.10/site-packages/openai/resources/fine_tuning/jobs/jobs.py:133\u001b[0m, in \u001b[0;36mJobs.create\u001b[0;34m(self, model, training_file, hyperparameters, integrations, seed, suffix, validation_file, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m     68\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FineTuningJob:\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    Creates a fine-tuning job which begins the process of creating a new model from\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    a given dataset.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/fine_tuning/jobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhyperparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mintegrations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mintegrations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjob_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJobCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFineTuningJob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pap/lib/python3.10/site-packages/openai/_base_client.py:1250\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1238\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1247\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1248\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1249\u001b[0m     )\n\u001b[0;32m-> 1250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pap/lib/python3.10/site-packages/openai/_base_client.py:931\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    924\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    929\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    930\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/pap/lib/python3.10/site-packages/openai/_base_client.py:1030\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1029\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1033\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1034\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1038\u001b[0m )\n","\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Model gpt-4o is not available for fine-tuning or does not exist.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}"]}],"source":["client.fine_tuning.jobs.create(\n","    training_file='file-KesfDbwQWsK19Rd9FNpYmm3a',\n","    model=\"gpt-4o-2024-08-06\"\n",")"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'messages' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 4\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmessages\u001b[49m},\n\u001b[1;32m      5\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am lookin\u001b[39m\u001b[38;5;124m\"\u001b[39m}],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# max_tokens = 150,\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m      8\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \n","\u001b[0;31mNameError\u001b[0m: name 'messages' is not defined"]}],"source":["response = client.chat.completions.create(\n","    model = \"gpt-4o\",\n","    messages = [\n","        {\"role\": \"system\", \"content\": messages},\n","        {\"role\": \"user\", \"content\": \"I am lookin\"}],\n","    # max_tokens = 150,\n","    temperature = 0.0,\n","    stream = False) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing: 100%|██████████| 10/10 [00:33<00:00,  3.36s/it]\n"]}],"source":["results_list = []\n","# 테스트 데이터: test\n","total_iterations = len(test)\n","\n","with tqdm(total=total_iterations, desc=\"Processing\") as pbar:\n","    for i in test:\n","        messages = [\n","            {\"role\": \"system\", \"content\": config['gptapi']['system_content']['role_summarize']},\n","            {\"role\": \"user\", \"content\": i},\n","        ]\n","        # 요약 결과를 받아오기\n","        response_sum = get_response(messages)\n","        messages.append({\"role\": \"assistant\", \"content\": response_sum})\n","        # 감정 분석 결과를 받아오기\n","        input_senti = config['gptapi']['assistant_content']['senti_categ']\n","        messages.append({\"role\": \"user\", \"content\": input_senti})\n","        response_senti = get_response(messages)\n","\n","        results_list.append({'original_message': i, 'summary': response_sum, 'sentiment': response_senti})\n","        pbar.update(1)\n","        time.sleep(1)\n","\n","results = pd.DataFrame(results_list)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPJdsknlGc3fMDAIl33vyWG","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
